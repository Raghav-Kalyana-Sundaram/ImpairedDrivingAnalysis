{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/#respond\n",
    "import cv2 # for capturing videos \n",
    "import math # for the mathematical operations required by our program \n",
    "import matplotlib.pyplot as plt # helps us plot the images\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image # does the preprocessing for the images\n",
    "import numpy as np # for more mathematical operations \n",
    "from keras.utils import np_utils\n",
    "from skimage.transform import resize # Does the resizing for the images \n",
    "\n",
    "# First we have to load the video and convert it into frames. \n",
    "count = 0\n",
    "videoFile = \"https://www.youtube.com/shorts/NjwktXZOCqs\"\n",
    "cap = cv2.VideoCapture(videoFile) # gets the video from the given path \n",
    "frameRate = cap.get(5) #frame rate\n",
    "x=1\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "print (\"Done!\")\n",
    "\n",
    "# Process prints done once the frames have been created. \n",
    "    \n",
    "count = 0\n",
    "videoFile = \"\"\n",
    "cap = cv2.VideoCapture(videoFile)\n",
    "frameRate = cap.get(5) #frame rate\n",
    "x=1\n",
    "while(cap.isOpened(https://www.youtube.com/shorts/NjwktXZOCqs)):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"test%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "print (\"Done!\")\n",
    "\n",
    "data = pd.read_csv('Classification.csv') # This reads the csv file \n",
    "data.head() # This prints the first five rows of the file \n",
    "# The mapping file contains the name of each image, or Image_ID, as well as the corresponding class, which is referenced in the model_class.json file. \n",
    "X = [] # Creates an empty array \n",
    "for img_name in data.Image_ID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X.append(img) # stores each image in array X\n",
    "X = np.array(X) # converts the list to an array \n",
    "\n",
    "# Now we need two things to train our model, training images, and their corresponding class \n",
    "\n",
    "test_image = []\n",
    "for img_name in test.Image_ID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    test_image.append(img)\n",
    "test_img = np.array(test_image)\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "train_y = np_utils.to_categorical(data.Class)\n",
    "test_y = np_utils.to_categorical(test.Class)\n",
    "\n",
    "# Using an input image of a shape that is 224 by 224 by 3 so are images need to be reshaped. \n",
    "image = []\n",
    "for i in range(0,X.shape[0]):\n",
    "    a = resize(X[i], preserve_range=True, output_shape=(224,224,3)).astype(int)\n",
    "    image.append(a)\n",
    "X = np.array(image)\n",
    "\n",
    "test_image = []\n",
    "for i in range(0,test_img.shape[0]):\n",
    "    a = resize(test_img[i], preserve_range=True, output_shape=(224,224)).astype(int)\n",
    "    test_image.append(a)\n",
    "test_image = np.array(test_image)\n",
    "\n",
    "# Preprocessing the image to allow the model to perform better. \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "X = preprocess_input(X, mode='tf')\n",
    "test_image = preprocess_input(test_image, mode='tf')\n",
    "# Checking a validation set to check the performance of the model on unseen images. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, train_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# using the VGG16 pretrained model for building the model. \n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout\n",
    "\n",
    "# Loading the VGG16 pretrained model and storing it as the base model \n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# We will make predictions using this model for x_train and x_valid, get the features, and then use the features to retrain the model. \n",
    "X_train = base_model.predict(X_train)\n",
    "X_valid = base_model.predict(X_valid)\n",
    "test_image = base_model.predict(test_image)\n",
    "# Reshaping X_train and X_valid to pass it through out nueral network. \n",
    "X_train = X_train.reshape(208, 7*7*512)\n",
    "X_valid = X_valid.reshape(90, 7*7*512)\n",
    "test_image = test_image.reshape(186, 7*7*512)\n",
    "\n",
    "# Preprocess the images and make them zero-centered to help the model to converge faster. \n",
    "train = X_train/X_train.max()\n",
    "X_valid = X_valid/X_train.max()\n",
    "test_image = test_image/test_image.max()\n",
    "\n",
    "# Building the model. \n",
    "model = Sequential()\n",
    "model.add(InputLayer((7*7*512,)))    # input layer\n",
    "model.add(Dense(units=1024, activation='sigmoid'))   # hidden layer\n",
    "model.add(Dropout(0.5))      # adding dropout\n",
    "model.add(Dense(units=512, activation='sigmoid'))    # hidden layer\n",
    "model.add(Dropout(0.5))      # adding dropout\n",
    "model.add(Dense(units=256, activation='sigmoid'))    # hidden layer\n",
    "model.add(Dropout(0.5))      # adding dropout\n",
    "model.add(Dense(3, activation='softmax'))            # output layer\n",
    "\n",
    "# Compiling the modle \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "class_weights = compute_class_weight('balanced',np.unique(data.Class), data.Class)  # computing weights of different classes\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]      # model check pointing based on validation loss\n",
    "\n",
    "#Training the model \n",
    "model.fit(train, y_train, epochs=100, validation_data=(X_valid, y_valid), class_weight=class_weights, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.hdf5\")\n",
    "model.compile(loss-'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "scores=model.evaluate(test_image, test_y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
